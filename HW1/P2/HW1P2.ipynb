{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dcbca6-f4f5-47b8-8aca-a03ab19abd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rslim/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchsummaryX import summary\n",
    "import sklearn\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import wandb\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device: ',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c038eaad-a585-46ac-8647-ea3c3156a9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PHONEME LIST\n",
    "PHONEMES = [\n",
    "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
    "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
    "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
    "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
    "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
    "            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2d14a2-c0ef-44c3-90e8-a3aee996986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this if you are running directly in kaggle notebook\n",
    "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
    "# !mkdir /root/.kaggle\n",
    "\n",
    "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "#     f.write('{\"username\":\"Replace this with your Kaggle Username\",\"key\":\"Replace this with your kaggle API key\"}')\n",
    "#     # Put your kaggle username & key here\n",
    "\n",
    "# !chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9da4bbb-3ac0-47f1-a424-ca6d11ccdc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"11785-hw1p2-f24\" + \".zip\"):\n",
    "    !kaggle competitions download -c 11785-hw1p2-f24\n",
    "\n",
    "if not os.path.exists(\"11785-f24-hw1p2\"):\n",
    "    !unzip -qo 11785-hw1p2-f24.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8fb9201-c058-467c-879a-4157389a7d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImplements a scanning MLP \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implements a scanning MLP \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cc1a55c-9970-406d-9827-1f81e84e7df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset class to load train and validation data\n",
    "# root = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# context = 20\n",
    "# # phonemes = PHONEMES\n",
    "# partition=\"train-clean-100\"\n",
    "\n",
    "# dataset_dir = \"11785-f24-hw1p2\"\n",
    "# dataset_path = os.path.join(root, dataset_dir)\n",
    "\n",
    "# mfcc_dir = os.path.join(dataset_path, partition, 'mfcc')\n",
    "# transcript_dir = os.path.join(dataset_path, partition, 'transcript')\n",
    "\n",
    "# # list files in self.mfcc_dir using os.listdir in sorted order\n",
    "# mfcc_names = []\n",
    "# files = os.listdir(mfcc_dir)\n",
    "# for file in files:\n",
    "#     mfcc_names.append(os.path.join(mfcc_dir, file))\n",
    "\n",
    "# transcript_names = []\n",
    "# files = os.listdir(transcript_dir)\n",
    "# for file in files:\n",
    "#     transcript_names.append(os.path.join(transcript_dir, file))\n",
    "\n",
    "# mfcc_names = sorted(mfcc_names)\n",
    "# transcript_names = sorted(transcript_names)\n",
    "\n",
    "# # Making sure that we have the same no. of mfcc and transcripts\n",
    "# assert len(mfcc_names) == len(transcript_names)\n",
    "\n",
    "# mfccs, transcripts = [], []\n",
    "\n",
    "# # Iterate through mfccs and transcripts\n",
    "# for i in range(len(mfcc_names)):\n",
    "#     # Load a single mfcc\n",
    "#     mfcc = np.load(mfcc_names[i])\n",
    "    \n",
    "#     # Do cepstral normalization of mfcc\n",
    "#     mfcc_mean = np.mean(mfcc, axis=0)\n",
    "#     mfcc_std = np.std(mfcc, axis=0)\n",
    "#     mfcc = (mfcc - mfcc_mean) / mfcc_std\n",
    "    \n",
    "#     transcript = np.load(transcript_names[i])[1:-1]  # remove [SOS] at beginning, and [EOS] at end efficiently.\n",
    "    \n",
    "#     mfccs.append(mfcc)\n",
    "#     transcripts.append(transcript)\n",
    "\n",
    "\n",
    "\n",
    "# mfccs = np.concatenate(mfccs, axis=0)\n",
    "# transcripts = np.concatenate(transcripts, axis=0)\n",
    "\n",
    "# mfccs = np.pad(mfccs, ((context, context), (0, 0)), mode='constant')\n",
    "# # The available phonemes in the transcript are of string data type\n",
    "# # But the neural network cannot predict strings as such.\n",
    "# # Hence, we map these phonemes to integers\n",
    "\n",
    "# # Map the phonemes to their corresponding list indexes in self.phonemes\n",
    "# phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(PHONEMES)}\n",
    "# transcripts = np.array([phoneme_to_index[phoneme] for phoneme in transcripts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195a48d7-d4ee-4216-9af2-b0780baf5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1071693-c44b-461a-bdce-7f358583400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcripts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93f17015-b2dc-4302-b616-1e32430ee105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load(mfcc_names[5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "505c8e6f-a41a-43ad-8125-308b4f41003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load(transcript_names[5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d1ce6fc-aa3c-4440-bccc-3314922b8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfccs[1:1+2*20+1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf41ca4f-2835-40cf-b605-516e9a1b4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfccs[1:1+2*20+1].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14fb0d6d-42e7-4071-a8c9-e71490491e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = 0\n",
    "# frames = mfccs[ind:ind + 2 * context + 1]\n",
    "# frames = frames.flatten()\n",
    "# print(frames.shape)\n",
    "# print(frames)\n",
    "# phonemes = transcripts[0]\n",
    "# print(phonemes.shape)\n",
    "# print(phonemes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97b461f6-0166-4ad7-adc0-3395c7d385b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(PHONEMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cde1c63-311a-447b-86b3-c714feb9102e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mfccs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d60f3e-790c-4bbc-886c-c520d4692d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b11ad-dc4d-4268-99c8-2898f5164dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56bd5bf3-4147-40d8-a560-8a82c0a5365f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to load train and validation data\n",
    "root = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, phonemes=PHONEMES, context=0, partition=\"train-clean-100\"):  # Feel free to add more arguments\n",
    "\n",
    "        self.context = context\n",
    "        self.phonemes = phonemes\n",
    "\n",
    "        dataset_dir = \"11785-f24-hw1p2\"\n",
    "        dataset_path = os.path.join(root, dataset_dir)\n",
    "        \n",
    "        self.mfcc_dir = os.path.join(dataset_path, partition, 'mfcc')\n",
    "        self.transcript_dir = os.path.join(dataset_path, partition, 'transcript')\n",
    "        \n",
    "        # list files in self.mfcc_dir using os.listdir in sorted order\n",
    "        mfcc_names = []\n",
    "        files = os.listdir(self.mfcc_dir)\n",
    "        for file in files:\n",
    "            mfcc_names.append(os.path.join(self.mfcc_dir, file))\n",
    "\n",
    "        transcript_names = []\n",
    "        files = os.listdir(self.transcript_dir)\n",
    "        for file in files:\n",
    "            transcript_names.append(os.path.join(self.transcript_dir, file))\n",
    "\n",
    "        mfcc_names = sorted(mfcc_names)\n",
    "        transcript_names = sorted(transcript_names)\n",
    "        \n",
    "        # Making sure that we have the same no. of mfcc and transcripts\n",
    "        assert len(mfcc_names) == len(transcript_names)\n",
    "\n",
    "        self.mfccs, self.transcripts = [], []\n",
    "\n",
    "        # Iterate through mfccs and transcripts\n",
    "        for i in range(len(mfcc_names)):\n",
    "            # Load a single mfcc\n",
    "            mfcc = np.load(mfcc_names[i])\n",
    "            \n",
    "            # Do cepstral normalization of mfcc\n",
    "            mfcc_mean = np.mean(mfcc, axis=0)\n",
    "            mfcc_std = np.std(mfcc, axis=0)\n",
    "            mfcc = (mfcc - mfcc_mean) / mfcc_std\n",
    "            \n",
    "            transcript = np.load(transcript_names[i])[1:-1]  # remove [SOS] at beginning, and [EOS] at end efficiently.\n",
    "            \n",
    "            self.mfccs.append(mfcc)\n",
    "            self.transcripts.append(transcript)\n",
    "\n",
    "        \n",
    "\n",
    "        self.mfccs = np.concatenate(self.mfccs, axis=0)\n",
    "        self.transcripts = np.concatenate(self.transcripts, axis=0)\n",
    "\n",
    "        assert len(self.mfccs) == len(self.transcripts)\n",
    "        \n",
    "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
    "        self.length = len(self.mfccs)\n",
    "\n",
    "        # self.mfcc is an array of the format (Frames x Features).\n",
    "        # Our goal is to recognize phonemes of each frame\n",
    "        # We can introduce context by padding zeros on top and bottom of self.mfcc\n",
    "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), mode='constant')\n",
    "        # The available phonemes in the transcript are of string data type\n",
    "        # But the neural network cannot predict strings as such.\n",
    "        # Hence, we map these phonemes to integers\n",
    "        \n",
    "        # Map the phonemes to their corresponding list indexes in self.phonemes\n",
    "        phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(self.phonemes)}\n",
    "        self.transcripts = np.array([phoneme_to_index[phoneme] for phoneme in self.transcripts])\n",
    "        # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n",
    "\n",
    "        # print(\"self.mfccs.shape \", self.mfccs.shape)\n",
    "        # print(\"self.transcripts.shape \", self.transcripts.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        frames = self.mfccs[ind:ind + 2 * self.context + 1]\n",
    "        frames = frames.flatten()\n",
    "        frames = torch.FloatTensor(frames)  # Convert to tensors\n",
    "        phonemes = torch.tensor(self.transcripts[ind])\n",
    "        return frames, phonemes\n",
    "\n",
    "# train_dataset = AudioDataset(root, context=0)\n",
    "# x, y = train_dataset[len(train_dataset)-1]\n",
    "# print(x)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf459f52-8153-431a-bf79-e79c51fab524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTestDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, phonemes=PHONEMES, context=0, partition=\"test-clean\"):\n",
    "\n",
    "        self.context = context\n",
    "        self.phonemes = phonemes\n",
    "        \n",
    "        dataset_dir = \"11785-f24-hw1p2\"\n",
    "        dataset_path = os.path.join(root, dataset_dir)\n",
    "\n",
    "        self.mfcc_dir = os.path.join(dataset_path, partition, 'mfcc')\n",
    "\n",
    "        # list files in self.mfcc_dir using os.listdir in sorted order\n",
    "        mfcc_names = []\n",
    "        files = os.listdir(self.mfcc_dir)\n",
    "        for file in files:\n",
    "            mfcc_names.append(os.path.join(self.mfcc_dir, file))\n",
    "\n",
    "        mfcc_names = sorted(mfcc_names)\n",
    "\n",
    "        self.mfccs = []\n",
    "        \n",
    "        # Iterate through mfccs and transcripts\n",
    "        for i in range(len(mfcc_names)):\n",
    "            # Load a single mfcc\n",
    "            mfcc = np.load(mfcc_names[i])\n",
    "            \n",
    "            # Do cepstral normalization of mfcc\n",
    "            mfcc_mean = np.mean(mfcc, axis=0)\n",
    "            mfcc_std = np.std(mfcc, axis=0)\n",
    "            mfcc = (mfcc - mfcc_mean) / mfcc_std\n",
    "            \n",
    "            self.mfccs.append(mfcc)\n",
    "\n",
    "        self.mfccs = np.concatenate(self.mfccs, axis=0)\n",
    "\n",
    "        self.length = len(self.mfccs)\n",
    "\n",
    "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), mode='constant')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        frames = self.mfccs[ind:ind + 2 * self.context + 1]\n",
    "        frames = frames.flatten()\n",
    "        frames = torch.FloatTensor(frames)  # Convert to tensors\n",
    "        return frames\n",
    "\n",
    "# test_dataset = AudioTestDataset(root)\n",
    "# x = test_dataset[len(test_dataset)-1]\n",
    "# print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82217df1-10ae-434c-95dd-d1e30f8252ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'epochs' : 5,\n",
    "    'batch_size' : 1024,\n",
    "    'context' : 20,\n",
    "    'init_lr' : 1e-3,\n",
    "    'architecture' : 'very-low-cutoff'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "511ffcc5-e2a5-406f-b7ba-6ba6097617ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = AudioDataset(root, context=config['context'], partition='train-clean-100')\n",
    "\n",
    "val_data = AudioDataset(root, context=config['context'], partition='dev-clean')\n",
    "\n",
    "test_data = AudioTestDataset(root, context=config['context'], partition='test-clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d88e3b7-588b-4b68-9c2a-b03950d04628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = train_data[len(train_data)-12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "920bbb16-45f4-42df-94a4-19227e207e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7863e35c-a418-4776-948b-c4821b32c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "586140f4-9191-4753-80d7-a081834d3074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size     :  1024\n",
      "Context        :  20\n",
      "Input size     :  1148\n",
      "Output symbols :  42\n",
      "Train dataset samples = 36091157, batches = 35246\n",
      "Validation dataset samples = 1928204, batches = 1884\n",
      "Test dataset samples = 1934138, batches = 1889\n"
     ]
    }
   ],
   "source": [
    "# Define dataloaders for train, val and test datasets\n",
    "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
    "# We shuffle train dataloader but not val & test dataloader. Why?\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    num_workers = 4,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    num_workers = 2,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = test_data,\n",
    "    num_workers = 2,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Batch size     : \", config['batch_size'])\n",
    "print(\"Context        : \", config['context'])\n",
    "print(\"Input size     : \", (2 * config['context'] + 1) * 28)\n",
    "print(\"Output symbols : \", len(PHONEMES))\n",
    "\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a8d3412-89b4-4c86-bf36-eb2fa38fec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2 * config['context'] + 1) * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53349623-ca0a-41a6-9932-45e546a787d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1148]) torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "# Testing code to check if your data loaders are working\n",
    "for i, data in enumerate(train_loader):\n",
    "    frames, phoneme = data\n",
    "    print(frames.shape, phoneme.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d66a214-e44d-41e1-9415-efd37983a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This architecture will make you cross the very low cutoff\n",
    "# However, you need to run a lot of experiments to cross the medium or high cutoff\n",
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_size, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a911007c-f4d2-40be-abd4-41ed88efc6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Layer                   Kernel Shape         Output Shape         # Params (K)      # Mult-Adds (M)\n",
      "====================================================================================================\n",
      "0_Linear                 [1148, 512]          [1024, 512]               588.29                 0.59\n",
      "1_ReLU                             -          [1024, 512]                    -                    -\n",
      "2_Linear                   [512, 42]           [1024, 42]                21.55                 0.02\n",
      "====================================================================================================\n",
      "# Params:    609.83K\n",
      "# Mult-Adds: 0.61M\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = (2 * config['context'] + 1) * 28\n",
    "model = Network(INPUT_SIZE, len(train_data.phonemes)).to(device)\n",
    "summary(model, frames.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc50127e-c195-4986-ba88-c5e6221093ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss function.\n",
    "# We use CE for multi-class classification\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['init_lr'])  # Defining optimizer\n",
    "# Recommended : Define scheduler for Learning Rate, \n",
    "# including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc.\n",
    "# You can refer to Pytorch documentation for more information on how to use them.\n",
    "\n",
    "# Is your training time very high?\n",
    "# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n",
    "# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c046d677-1f6b-48fc-9bcc-f1b25fa94f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a2b0b-2d76-4c63-8c20-a806af0f864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "\n",
    "    model.train()\n",
    "    tloss, tacc = 0, 0  # Monitoring loss and accuracy\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    for i, (frames, phonemes) in enumerate(dataloader):\n",
    "        \n",
    "        # Initialize Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device (ideally GPU)\n",
    "        frames = frames.to(device)\n",
    "        phonemes = phonemes.to(device)\n",
    "\n",
    "        # forward propagation\n",
    "        logits = model(frames)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = criterion(logits, phonemes)\n",
    "\n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent\n",
    "        optimizer.step()\n",
    "\n",
    "        tloss += loss.item()\n",
    "        # print(\"logits\", logits)\n",
    "        # print(\"logits.shape\", logits.shape)\n",
    "        # print(\"torch.argmax(logits, dim=1)\", torch.argmax(logits, dim=1))\n",
    "        # print(\"torch.argmax(logits, dim=1).shape\", torch.argmax(logits, dim=1).shape)\n",
    "        # print(\"torch.argmax(logits, dim=1)==phonemes\", torch.argmax(logits, dim=1)==phonemes)\n",
    "        tacc += torch.sum(torch.argmax(logits, dim=1) == phonemes).item() / logits.shape[0]\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
    "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "        # release memory\n",
    "        del frames, phonemes, logits\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    tloss /= len(dataloader)\n",
    "    tacc /= len(dataloader)\n",
    "\n",
    "    return tloss, tacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e93b56-ad68-440d-8052-d0e9baa6a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "\n",
    "    model.eval()  # set model in evaluation mode\n",
    "    vloss, vacc = 0, 0  # monitoring loss and accuracy\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    for i, (frames, phonemes) in enumerate(dataloader):\n",
    "\n",
    "        # move data to device (ideally a GPU)\n",
    "        frames = frames.to(device)\n",
    "        phonemes = phonemes.to(device)\n",
    "\n",
    "        # makes sure that there are no gradients computed as we are not training the model now\n",
    "        with torch.inference_mode():\n",
    "            # forward propagation\n",
    "            logits = model(frames)\n",
    "\n",
    "            # loss calculation\n",
    "            loss = criterion(logits, phonemes)\n",
    "\n",
    "        vloss += loss.item()\n",
    "        vacc += torch.sum(torch.argmax(logits, dim=1) == phonemes).item() / logits.shape[0]\n",
    "\n",
    "        # Do you think we need loss.backward() and optimizer.step() here?\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
    "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
    "\n",
    "        # Release memory\n",
    "        del frames, phonemes, logits\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    vloss /= len(dataloader)\n",
    "    vacc /= len(dataloader)\n",
    "\n",
    "    return vloss, vacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc33a012-ef86-4154-9e86-a906f807c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/rslim/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreinaldymaslim\u001b[0m (\u001b[33mreinaldymaslim-nanyang-technological-university-singapore\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"c62dd63a448dbe45c79277111818735bed8f5244\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "627f4b86-51da-46f4-b3b0-81312592e51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rslim/proj/dl/dl2/HW1/P2/wandb/run-20250729_141202-x292tgk6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2/runs/x292tgk6' target=\"_blank\">first-run</a></strong> to <a href='https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2' target=\"_blank\">https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2/runs/x292tgk6' target=\"_blank\">https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2/runs/x292tgk6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your wandb run\n",
    "run = wandb.init(\n",
    "    name    = \"first-run\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n",
    "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"hw1p2\", ### Project should be created in your wandb account\n",
    "    config  = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b301ab9-3588-4cea-8292-7ebaa50f308a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/rslim/proj/dl/dl2/HW1/P2/wandb/run-20250729_141202-x292tgk6/files/model_arch.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save your model architecture as a string with str(model)\n",
    "model_arch  = str(model)\n",
    "\n",
    "### Save it in a txt file\n",
    "arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "file_write  = arch_file.write(model_arch)\n",
    "arch_file.close()\n",
    "\n",
    "### log it in your wandb run with wandb.save()\n",
    "wandb.save('model_arch.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be2568a7-3614-4317-8134-95c863d2b577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(model, train_loader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62016ca1-1835-4408-98e7-64bce00a9c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4732c-a0d1-4933-af5c-2a8a0156ca5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b68f9a3-8be2-4194-accf-19e2cd57f81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 67.9945%\tTrain Loss 1.0404\t Learning Rate 0.0010000\n",
      "\tVal Acc 67.9164%\tVal Loss 1.0387\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 69.7877%\tTrain Loss 0.9782\t Learning Rate 0.0010000\n",
      "\tVal Acc 68.6853%\tVal Loss 1.0093\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 70.4589%\tTrain Loss 0.9549\t Learning Rate 0.0010000\n",
      "\tVal Acc 69.5162%\tVal Loss 0.9861\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 70.8440%\tTrain Loss 0.9415\t Learning Rate 0.0010000\n",
      "\tVal Acc 69.5022%\tVal Loss 0.9832\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Acc 71.0926%\tTrain Loss 0.9329\t Learning Rate 0.0010000\n",
      "\tVal Acc 69.8510%\tVal Loss 0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Iterate over number of epochs to train and evaluate your model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
    "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc       = eval(model, val_loader)\n",
    "\n",
    "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
    "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
    "\n",
    "    ### Log metrics at each epoch in your run\n",
    "    # Optionally, you can log at each batch inside train/eval functions\n",
    "    # (explore wandb documentation/wandb recitation)\n",
    "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
    "               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
    "\n",
    "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
    "\n",
    "### Finish your wandb run\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050dee8-ef3c-4e64-94d5-10b782d2abc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7cfd9a8a-fdf9-45ef-b867-39e2e4c31810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    ### What you call for model to perform inference?\n",
    "    model.eval() # TODO train or eval?\n",
    "\n",
    "    ### List to store predicted phonemes of test data\n",
    "    test_predictions = []\n",
    "\n",
    "    ### Which mode do you need to avoid gradients?\n",
    "    with torch.inference_mode(): # TODO\n",
    "\n",
    "        for i, mfccs in enumerate(tqdm(test_loader)):\n",
    " \n",
    "            mfccs   = mfccs.to(device)\n",
    "\n",
    "            logits  = model(mfccs)\n",
    "\n",
    "            # print(logits)\n",
    "\n",
    "            ### Get most likely predicted phoneme with argmax\n",
    "            predicted_phonemes_indices = torch.argmax(logits, dim=1)\n",
    "            # print(predicted_phonemes)\n",
    "            # print(predicted_phonemes.shape)\n",
    "            predicted_phonemes = [PHONEMES[i] for i in predicted_phonemes_indices.cpu().numpy()]\n",
    "            \n",
    "            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n",
    "            test_predictions.extend(predicted_phonemes)\n",
    "\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e856ed4-d32f-45d9-a65e-a0d93ad59ab5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                     | 0/1889 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                                                                                                                                                             | 1/1889 [00:00<03:20,  9.40it/s]\u001b[A\n",
      "  3%|████▍                                                                                                                                                                      | 49/1889 [00:00<00:06, 278.55it/s]\u001b[A\n",
      "  5%|████████▊                                                                                                                                                                  | 98/1889 [00:00<00:04, 373.07it/s]\u001b[A\n",
      "  8%|█████████████▎                                                                                                                                                            | 148/1889 [00:00<00:04, 417.54it/s]\u001b[A\n",
      " 10%|█████████████████▋                                                                                                                                                        | 197/1889 [00:00<00:03, 442.13it/s]\u001b[A\n",
      " 13%|█████████████████████▉                                                                                                                                                    | 244/1889 [00:00<00:03, 451.37it/s]\u001b[A\n",
      " 16%|██████████████████████████▍                                                                                                                                               | 294/1889 [00:00<00:03, 464.48it/s]\u001b[A\n",
      " 18%|██████████████████████████████▉                                                                                                                                           | 344/1889 [00:00<00:03, 474.53it/s]\u001b[A\n",
      " 21%|███████████████████████████████████▎                                                                                                                                      | 392/1889 [00:00<00:03, 475.57it/s]\u001b[A\n",
      " 23%|███████████████████████████████████████▋                                                                                                                                  | 441/1889 [00:01<00:03, 478.28it/s]\u001b[A\n",
      " 26%|████████████████████████████████████████████▏                                                                                                                             | 491/1889 [00:01<00:02, 482.09it/s]\u001b[A\n",
      " 29%|████████████████████████████████████████████████▌                                                                                                                         | 540/1889 [00:01<00:02, 481.64it/s]\u001b[A\n",
      " 31%|█████████████████████████████████████████████████████                                                                                                                     | 589/1889 [00:01<00:02, 478.26it/s]\u001b[A\n",
      " 34%|█████████████████████████████████████████████████████████▎                                                                                                                | 637/1889 [00:01<00:02, 475.51it/s]\u001b[A\n",
      " 36%|█████████████████████████████████████████████████████████████▋                                                                                                            | 686/1889 [00:01<00:02, 479.13it/s]\u001b[A\n",
      " 39%|██████████████████████████████████████████████████████████████████                                                                                                        | 734/1889 [00:01<00:02, 475.95it/s]\u001b[A\n",
      " 41%|██████████████████████████████████████████████████████████████████████▍                                                                                                   | 782/1889 [00:01<00:02, 471.42it/s]\u001b[A\n",
      " 44%|██████████████████████████████████████████████████████████████████████████▋                                                                                               | 830/1889 [00:01<00:02, 469.52it/s]\u001b[A\n",
      " 46%|███████████████████████████████████████████████████████████████████████████████                                                                                           | 878/1889 [00:01<00:02, 467.53it/s]\u001b[A\n",
      " 49%|███████████████████████████████████████████████████████████████████████████████████▏                                                                                      | 925/1889 [00:02<00:02, 466.89it/s]\u001b[A\n",
      " 51%|███████████████████████████████████████████████████████████████████████████████████████▍                                                                                  | 972/1889 [00:02<00:01, 466.08it/s]\u001b[A\n",
      " 54%|███████████████████████████████████████████████████████████████████████████████████████████▏                                                                             | 1019/1889 [00:02<00:01, 466.24it/s]\u001b[A\n",
      " 56%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                                                         | 1066/1889 [00:02<00:01, 464.73it/s]\u001b[A\n",
      " 59%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                     | 1114/1889 [00:02<00:01, 465.45it/s]\u001b[A\n",
      " 61%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 1161/1889 [00:02<00:01, 456.56it/s]\u001b[A\n",
      " 64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                             | 1208/1889 [00:02<00:01, 456.94it/s]\u001b[A\n",
      " 66%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                        | 1255/1889 [00:02<00:01, 459.39it/s]\u001b[A\n",
      " 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                    | 1301/1889 [00:02<00:01, 459.08it/s]\u001b[A\n",
      " 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                | 1348/1889 [00:02<00:01, 461.06it/s]\u001b[A\n",
      " 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                            | 1395/1889 [00:03<00:01, 457.86it/s]\u001b[A\n",
      " 76%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 1441/1889 [00:03<00:00, 457.47it/s]\u001b[A\n",
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                    | 1487/1889 [00:03<00:00, 454.28it/s]\u001b[A\n",
      " 81%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                               | 1533/1889 [00:03<00:00, 454.34it/s]\u001b[A\n",
      " 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                           | 1580/1889 [00:03<00:00, 455.03it/s]\u001b[A\n",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                       | 1626/1889 [00:03<00:00, 451.98it/s]\u001b[A\n",
      " 89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 1675/1889 [00:03<00:00, 462.53it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 1723/1889 [00:03<00:00, 466.45it/s]\u001b[A\n",
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 1770/1889 [00:03<00:00, 465.01it/s]\u001b[A\n",
      " 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌      | 1817/1889 [00:03<00:00, 462.59it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1889/1889 [00:04<00:00, 452.74it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "predictions = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c1a082f-0865-4284-9324-be9668f11550",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create CSV file with predictions\n",
    "with open(\"./submission.csv\", \"w+\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(\"{},{}\\n\".format(i, predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "752529eb-7f9e-4d68-9bf8-c3e0dec3f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = os.path.dirname(os.path.abspath(\"__file__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89efd76f-c7cc-4cce-b24d-4959ba849fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset class to load train and validation data\n",
    "# dataset_dir = \"11785-f24-hw1p2\"\n",
    "# root = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "# dataset_path = os.path.join(root, dataset_dir)\n",
    "# train_and_test = [\"train-clean-100\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "147b66c6-48bf-45d7-b469-14cf87256cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc_dir = [os.path.join(dataset_path, folder, 'mfcc') for folder in train_and_test]\n",
    "# transcript_dir = [os.path.join(dataset_path, folder, 'transcript') for folder in train_and_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cea1fd27-16a9-4137-b2ae-e77d2da02daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfcc_names = []\n",
    "# for path in mfcc_dir:\n",
    "#     files = os.listdir(path)\n",
    "#     for file in files:\n",
    "#         mfcc_names.append(os.path.join(path, file))\n",
    "# mfcc_names = sorted(mfcc_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0b3b9bf-9ed1-4cc8-9ec9-9b277d8a359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript_names = []\n",
    "# for path in transcript_dir:\n",
    "#     files = os.listdir(path)\n",
    "#     for file in files:\n",
    "#         transcript_names.append(os.path.join(path, file))\n",
    "# transcript_names = sorted(transcript_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52cdd670-e219-40c6-beb1-6cc0b3029a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfccs = []\n",
    "# transcripts = []\n",
    "# assert len(mfcc_names) == len(transcript_names)\n",
    "# for i in range(len(mfcc_names)):\n",
    "#     mfccs.append(np.load(mfcc_names[i]))\n",
    "#     transcripts.append(np.load(transcript_names[i])[1:-1])\n",
    "# print(len(mfccs))\n",
    "# print(len(transcripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1556c7-3b55-45ca-bb81-de7ac34892a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b1e58190-9b37-4318-887c-d8f7c85a1d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phoneme_to_index = {phoneme: idx for idx, phoneme in enumerate(PHONEMES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7f02c40-a60e-45b9-9fc3-ec456c02e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phoneme_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f32cedad-f3c7-4bf3-935f-8d0c3cc8568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcripts = np.array([phoneme_to_index[phoneme] for phoneme in transcripts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71aa9aaf-6dc4-4bec-a9c4-5847ca2b438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(transcripts==12)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98a06064-4d93-494a-8dbf-fbb99b67b086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇█</td></tr><tr><td>train_loss</td><td>█▄▂▂▁</td></tr><tr><td>val_acc</td><td>▁▄▇▇█</td></tr><tr><td>valid_loss</td><td>█▅▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.001</td></tr><tr><td>train_acc</td><td>71.0926</td></tr><tr><td>train_loss</td><td>0.93287</td></tr><tr><td>val_acc</td><td>69.85102</td></tr><tr><td>valid_loss</td><td>0.97223</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">first-run</strong> at: <a href='https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2/runs/x292tgk6' target=\"_blank\">https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2/runs/x292tgk6</a><br> View project at: <a href='https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2' target=\"_blank\">https://wandb.ai/reinaldymaslim-nanyang-technological-university-singapore/hw1p2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250729_141202-x292tgk6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Finish your wandb run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ee1642b-5b4c-41f0-b3bb-8d9668945855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 19.4M/19.4M [00:03<00:00, 5.91MB/s]\n",
      "Successfully submitted to 11785 HW1P2 Fall 2024"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c 11785-hw1p2-f24 -f ./submission.csv -m \"try submit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d0cc4-f65b-4d23-986a-9aaf65c0bed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b5bab-5322-4d18-b1d9-d1398d2f8c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
